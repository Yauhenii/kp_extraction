{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfe6830",
   "metadata": {},
   "source": [
    "# Using Project Debater services for analyzing and finding insights in the survey data \n",
    "When you have a large collection of texts representing people’s opinions (such as product reviews, survey answers or  social media), it is difficult to understand the key issues that come up in the data. Going over thousands of comments is prohibitively expensive.  Existing automated approaches are often limited to identifying recurring phrases or concepts and the overall sentiment toward them, but do not provide detailed or actionable insights.\n",
    "\n",
    "In this tutorial you will gain hands-on experience in using Project Debater services for analyzing and deriving insights from open-ended answers.  \n",
    "\n",
    "The data we will use is a community survey conducted in the city of Austin in the years 2016 and 2017 (https://data.world/cityofaustin/mf9f-kvkk). In this survey, the citizens of Austin where asked \"If there was ONE thing you could share with the Mayor regarding the City of Austin (any comment, suggestion, etc.), what would it be?\". \n",
    "\n",
    "We will analyze their open-ended answers in different ways by using four Debater services, the *Argument Quality* service, the *Key Point Analysis (KPA)* service, the *Term Wikifier* service and the *Term Relater* service, and we will see how they can be combined into a powerful text analysis tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69a6d5",
   "metadata": {},
   "source": [
    "## 1. Run *Key Point Analysis* on 1000 randomly selected sentences from 2016 survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d1023",
   "metadata": {},
   "source": [
    "### 1.1 Read random sample of 1000 sentences from 2016 comments\n",
    "Let's take a look at the first 5 lines in the *dataset_austin_sentences.csv* file, which holds the Austin survey dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07219d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,text,district,year\n",
      "\n",
      "1,\"Dissatisfied traffic and with traffic, timing of street lights.\",7,2016\n",
      "\n",
      "2,EXTREMELY dissatisfied with cit govt.,7,2016\n",
      "\n",
      "3,\"interfering in local businesses (Uber/Lyft, income property owners).\",7,2016\n",
      "\n",
      "4,\"Also, extremely dissatisfied with all the free handouts to people who are perfectly capable of earning their own money.\",7,2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open('./dataset_austin_sentences.csv', 'r')\n",
    "lines = file.readlines()\n",
    "print('\\n'.join(lines[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a0985",
   "metadata": {},
   "source": [
    "The file has all the survey answers after they were split into sentences. Each row in the file corresponds to a single sentence. Each row has the following attributes: \\['id', 'text', 'district','year'\\]. We will first read the attached csv file into the 'sentences' variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab275f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "with open('./dataset_austin_sentences.csv') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    sentences = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514dd2e9",
   "metadata": {},
   "source": [
    "Let's have a look at the content *sentences* variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0eae50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6274 sentences in the dataset\n",
      "Each sentence is a dictionary with the following keys: odict_keys(['id', 'text', 'district', 'year'])\n"
     ]
    }
   ],
   "source": [
    "print('There are %d sentences in the dataset' % len(sentences))\n",
    "print('Each sentence is a dictionary with the following keys: %s' % str(sentences[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1107f",
   "metadata": {},
   "source": [
    "Let's select only the sentences from the 2016 survey and randomly sample 1000 out of them. The *Key Point Analysis* service is able to run over hundreds of thousands of sentences, however since the computation is heavy in resources (particularly GPUs) the trial version is limited to 1000 sentences. Using a random.seed(0) is important since we already prepared a hot-cache over these sentences for a quicker *Key Point Analysis* run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab0f6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3005 sentences in the 2016 survey\n"
     ]
    }
   ],
   "source": [
    "sentences_2016 = [sentence for sentence in sentences if sentence['year'] == '2016']\n",
    "print('There are %d sentences in the 2016 survey' % len(sentences_2016))\n",
    "random.seed(0)\n",
    "random_sample_sentences_2016 = random.sample(sentences_2016, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfa3ab",
   "metadata": {},
   "source": [
    "### 1.2 Run *Key Point Analysis* on the random sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d410faaa",
   "metadata": {},
   "source": [
    "Key point analysis is a novel and promising approach for summarization, with an important quantitative angle. This service summarizes a collection of comments on a given topic as a small set of key points. The salience of each key point is given by the number of its matching sentences in the given comments.\n",
    "\n",
    "Before running the *Key Point Analysis* service we first need to initialize our client.  The DebaterApi object supplies the clients for the various Debater services.   The clients print information using the logger and a suitable verbosity level is should be set. The DebaterApi object is configured with an API key. It should be  retrieved from the Project Debater Early Access Program site.  In this case it is passed by the enviroment variable *DEBATER_API_KEY*.  We then obtain the keypoint client from the DebaterAPI object.\n",
    "\n",
    "The *Key Point Analysis* service stores the data (and results cache) in a domain. A user can create several domains, one for each dataset. Domains are only accessible to the user who created them.  In this tutorial, we will run all *Key Point Analysis* jobs in the same domain named 'austin_demo'.\n",
    "\n",
    "Full documentation of the *Key Point Analysis* service can be found [here](https://early-access-program.debater.res.ibm.com/docs/services/keypoints/keypoints_pydoc.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7708b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "from austin_utils import init_logger\n",
    "import os\n",
    "\n",
    "init_logger()\n",
    "api_key = os.environ['DEBATER_API_KEY']\n",
    "debater_api = DebaterApi(apikey=api_key)\n",
    "keypoints_client = debater_api.get_keypoints_client()\n",
    "domain = 'austin_demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d0ffb",
   "metadata": {},
   "source": [
    "Exercise 1:\n",
    "\n",
    "Let's define a method named *run_kpa*. The method receives a list of sentences (each sentence is a dictionary with the following keys: 'id','text') and runs *Key Point Analysis* on these sentences. The method also receives the *run_params* parameter, which enable us to customize and affect the *Key Point Analysis* operation.\n",
    "\n",
    "In order to run *Key Point Analysis*, we need to:\n",
    "\n",
    "1. Upload the comments into a domain using the **keypoints_client.upload_comments(domain=domain, comments_ids=sentences_ids, comments_texts=sentences_texts, dont_split=True)** method. This method receives the domain, a list of comment_ids and a list of comment_texts. By default, when uploading comments into a domain, the *Key Point Analysis* service splits the comments into sentences by default and runs a minor cleansing on the sentences. Since we already splitted the comments into sentences ourselves and we want to *Key Point Analysis* service to use them as is, we will set the *dont_split* parameter to True.\n",
    "\n",
    "2. Wait till all comments in the domain are processed using the **keypoints_client.wait_till_all_comments_are_processed(domain=domain)** method.\n",
    "\n",
    "3. Start a *Key Point Analysis* job using the **future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids, run_params=run_params)** method. This method receives the domain, a list of comment_ids and a *run_params*. The run_params is a dictionary with various parameters for customizing the job. The job runs in an async manner therefore the method returns a future object.\n",
    "\n",
    "4. Use the returned future and wait till results are available using the **kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)** method. The method waits for the job to finish and eventually returns the result. The result is a dictionary containing the key points (sorted descendingly according to number of matched sentences) and for each key point has a list of matched sentences (sorted descendingly according to their match score). An additional 'none' key point is added which holds all the sentences that don't match any key point.\n",
    "\n",
    "Our run_kpa method will return this result dictionary. It will also return the unique identifirt for this analysis called *job_id* retreived from the future. We will need this job_id in a following exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2727bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, run_params):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain, \n",
    "                                     comments_ids=sentences_ids, \n",
    "                                     comments_texts=sentences_texts, \n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain=domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, \n",
    "                                                    comments_ids=sentences_ids, \n",
    "                                                    run_params=run_params)\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, \n",
    "                                   polling_timout_secs=5)\n",
    "    \n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9501bda",
   "metadata": {},
   "source": [
    "We will now use the method you implemented and run over the random sample and print the result. In order to limit the number of key points in the result to 20, we will use *run_params={'n_top_kps': 20}*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac76d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:57:53,087 [INFO] keypoints_client.py 318: uploading 1000 comments in batches\n",
      "2022-05-11 10:57:53,091 [INFO] keypoints_client.py 246: client calls service (post): https://keypoint-matching-backend.debater.res.ibm.com/comments\n",
      "2022-05-11 10:57:54,827 [INFO] keypoints_client.py 335: uploaded 1000 comments, out of 1000\n",
      "2022-05-11 10:57:54,829 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/comments\n",
      "2022-05-11 10:58:00,040 [INFO] keypoints_client.py 347: domain: austin_demo, comments status: {'processed_comments': 1000, 'processed_sentences': 1000, 'pending_comments': 0}\n",
      "2022-05-11 10:58:00,041 [INFO] keypoints_client.py 246: client calls service (post): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:06,440 [INFO] keypoints_client.py 409: started a kp analysis job - domain: austin_demo, job_id: 627b7a9e45f8cd7a679ffcb8\n",
      "2022-05-11 10:58:06,441 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:12,026 [INFO] keypoints_client.py 590: job_id 627b7a9e45f8cd7a679ffcb8 is pending\n",
      "2022-05-11 10:58:17,032 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:18,003 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:58:23,007 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:24,028 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:58:29,033 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:30,129 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:58:35,135 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:36,167 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:58:41,175 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:42,206 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:58:47,212 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:48,149 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:58:53,155 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:58:54,163 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:58:59,168 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:00,215 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:05,222 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:06,218 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:11,223 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:12,279 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:17,286 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:18,325 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 0, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |--------------------------------------------------| 0.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:23,332 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:24,300 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 2, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |█████---------------------------------------------| 10.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:29,306 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:30,319 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 3, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |███████-------------------------------------------| 15.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:35,330 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:36,387 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 11, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |███████████████████████████-----------------------| 55.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:41,389 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:42,489 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 15, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |█████████████████████████████████████-------------| 75.0% Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:47,494 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:50,256 [INFO] keypoints_client.py 594: job_id 627b7a9e45f8cd7a679ffcb8 is running, progress: {'total_stages': 1, 'stage_1': {'inferred_batches': 20, 'total_batches': 20, 'batch_size': 2000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1/1: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:59:55,264 [INFO] keypoints_client.py 246: client calls service (get): https://keypoint-matching-backend.debater.res.ibm.com/kp_extraction\n",
      "2022-05-11 10:59:56,781 [INFO] keypoints_client.py 597: job_id 627b7a9e45f8cd7a679ffcb8 is done, returning result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 2016 coverage: 36.02\n",
      "Random sample 2016 key points:\n",
      "39 - We need better mass transit!\n",
      "\t- Need more bus routes!\n",
      "\t- NEED BETTER PUBLIC TRANSPORTATION\n",
      "37 - Traffic problems getting worse\n",
      "\t- Traffic is horrible in Austin!\n",
      "\t- FIX THE TRAFFIC, IT HAS GOTTEN WORSE OVER THE PAST 5 YEARS\n",
      "33 - Austin is not affordable.\n",
      "\t- Do something to change the fact that Austin is becoming too expensive to reside in\n",
      "\t- The cost of living in Austin has sky-rocketed, and local businesses are not able or\n",
      "\t  willing to meet the financial needs of the lower-middle class.\n",
      "26 - REDUCING TRAFFIC NEEDS TO BE PRIORITY.\n",
      "\t- The traffic problems need to be fixed asap.\n",
      "\t- Traffic must get better.\n",
      "22 - Better infrastructure of roads.\n",
      "\t- Why then, for the sake of growth, prosperity and infrastructure in general is there only\n",
      "\t  ONE major freeway intersecting this fast-growing and fabulous city?\n",
      "\t- need better growth planning mainly roads\n",
      "21 - Affordable housing is very important.\n",
      "\t- Affordable housing is crucial, & keeping seniors in their homes is part of that challenge!\n",
      "\t- Affordable housing MUST become a reality/ahora!\n",
      "20 - Homeowners taxes are a problem.\n",
      "\t- Taxes have gone out of control!!\n",
      "\t- Cost of living here is to high & tax for my home is to high.\n",
      "15 - Please stop adding new fees, costs, etc.\n",
      "\t- Remove all the extra charges, fees, etc.\n",
      "\t- Quit letting small radical groups like SOS stop needed projects and drive the cost up on\n",
      "\t  everything with their environmental blanket.\n",
      "14 - Provide public transportation to prevent traffic overflow.\n",
      "\t- Work with the state to improve transportation to ease congestion.\n",
      "\t- If the train traveled to more locations there would be more people on the train and less\n",
      "\t  traffic.\n",
      "14 - Wastewater fees are way too high.\n",
      "\t- Also, the cost of my water bill is insanely high and I am about to protest it!\n",
      "\t- The cost or water is excessive and way too high.\n",
      "11 - It's eradic and dangerous.\n",
      "\t- The number of homeless has become extreme and is a danger (UT student death).\n",
      "\t- Too dark, dangerous and curvy for night driving.\n",
      "9 - electric rates are too high\n",
      "\t- Rates are to high\n",
      "\t- HIGH TAX HIGH ELECTRIC AND WINTER RATES.\n",
      "8 - Attract a most diverse population to Austin\n",
      "\t- Affordability and diversity (in culture and age groups) are extremely important to\n",
      "\t  keeping Austin the vibrant, egalitarian city it is.\n",
      "\t- This is the reason why Austin currently has almost half the number of Black people here\n",
      "\t  as when I was growing up.\n",
      "8 - Infrastructure can't support rapid growth.\n",
      "\t- It cannot handle the high number of people!!\n",
      "\t- It is growing too quickly.\n",
      "8 - MAKE DEVELOPMENT EASIER AND FASTER.\n",
      "\t- Have permits for new commercial build outs be processed faster.\n",
      "\t- Let's make it easy to get here on a global scale.\n",
      "7 - KEEP THE CITY AFFORDABLE.\n",
      "\t- WORK TO MAKE THE CITY AFFORDABLE TO LIVE IN\n",
      "\t- THE CITY OF AUSTIN NEEDS TO BE MORE AFFORDABLE IN ORDER FOR LONG TIME RESIDENTS OF\n",
      "\t  AUSTIN TO BE ABLE TO REMAIN IN AUSTIN\n",
      "7 - Taxes/expenses go up, services go down.\n",
      "\t- Cost of housing, taxes, utilities are killing everything that was good about Austin.\n",
      "\t- Your conservation programs take our money and give it back to us and raising our cost of\n",
      "\t  utilities and taxes.\n",
      "5 - Austin should be pro-growth.\n",
      "\t- Let's manage our growth!\n",
      "\t- BETTER PLAN FOR GROWTH BEFORE BRINGING IN NEW BUSINESSES.\n",
      "5 - Better training and sensitivity is needed.\n",
      "\t- Better training for the 3-1-1 operators.\n",
      "\t- NEED TO IMPROVE POLICE ACCOUNTABILITY AND CURB ENTRIFICATION\n",
      "4 - CONSERVATION OF WATER AND PROVIDE GREEN SPACE\n",
      "\t- That combined with quality and preservation of outdoor spaces that make Austin such an\n",
      "\t  attractive location and place to live.\n",
      "\t- Austin is a unique city with exceptional green space in spite of its growth.\n"
     ]
    }
   ],
   "source": [
    "from austin_utils import print_results\n",
    "\n",
    "kpa_result_random_1000_2016, _ = run_kpa(random_sample_sentences_2016, {'n_top_kps': 20})\n",
    "print_results(kpa_result_random_1000_2016, n_sentences_per_kp=2, title='Random sample 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e73cde",
   "metadata": {},
   "source": [
    "## 2. Run *Key Point Analysis* on 1000 top quality sentences from 2016 survey\n",
    "### 2.1 Select top 1000 sentences from 2016 data using the *Argument Quality* service\n",
    "The answers in the Austin Survey dataset vary in length, style and quality. Selecting the sentences randomly may lead to running over many sentences that are not very informative. Running over the randomly selected sentences reached a 28.36% coverage. This means that only 28.36% of the sentences matched a key point. In order to improve the coverage and the quality of our results, we will now run over higher quality sentences and select the 1000 sentences with the highest *Argument Quality* score. The *Argument Quality* service receives pairs of \\[sentence, topic\\] and returns a score indicating whether the sentence is phrased in grammatically correct, clear and concise language.   The ranking of the quality is based on the machine learning model, which was trained on human assesments of over 30,000 arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ade02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_top_and_bottom_k_sentences\n",
    "\n",
    "def get_top_quality_sentences(sentences, top_k, topic):    \n",
    "    arg_quality_client = debater_api.get_argument_quality_client()\n",
    "    sentences_topic = [{'sentence': sentence['text'], 'topic': topic} for sentence in sentences]\n",
    "    arg_quality_scores = arg_quality_client.run(sentences_topic)\n",
    "    sentences_and_scores = zip(sentences, arg_quality_scores)\n",
    "    sentences_and_scores_sorted = sorted(sentences_and_scores, key=lambda x: x[1], reverse=True)\n",
    "    sentences_sorted = [sentence for sentence, _ in sentences_and_scores_sorted]\n",
    "    print_top_and_bottom_k_sentences(sentences_sorted, 10)\n",
    "    return sentences_sorted[:top_k]\n",
    "\n",
    "sentences_2016_top_1000_aq = get_top_quality_sentences(sentences_2016, 1000, 'Austin is a great place to live')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a83fc7",
   "metadata": {},
   "source": [
    "### 2.2 Run *Key Point Analysis* over the selected sentences\n",
    "We will now run the *run_kpa* method over the top 1000 quality sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016, _ = run_kpa(sentences_2016_top_1000_aq, {'n_top_kps': 20})\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c2cc0",
   "metadata": {},
   "source": [
    "### 2.3 Customize key point analysis\n",
    "It is possible to costumize and affect the analysis by passing different parameters in the *run_params* dictionary. In this subsection we will see few examples.\n",
    "\n",
    "#### 2.3.1 Hierarchical key points\n",
    "Often, few key points address a similar topic. In order to get an even clearer sumamry of the data, we can group similar key points together using the *Hierarchical Key Points* feature. To acrivate it we add two additional parameters to run_params: {'perform_kp_hierarchy': True, 'kp_hierarchy_threshold': 0.3}. *perform_kp_hierarchy* activates the feature and *kp_hierarchy_threshold* sets a threshold for grouping similar key points. The lower the threshold, more key points are grouped with lower similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016, _ = run_kpa(sentences_2016_top_1000_aq, \n",
    "                                    {'n_top_kps': 20, 'perform_kp_hierarchy': True, 'kp_hierarchy_threshold': 0.3})\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016, hierarchical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f89d3",
   "metadata": {},
   "source": [
    "#### 2.3.2 Increase coverage by decreasing the matching threshold\n",
    "Running over higher quality sentences we managed to increase our coverage to 41.05%. In order to increase the coverage more, we will add another parameter to the run_params called *mapping_threshold*. \n",
    "\n",
    "The mapping_threshold is responsible of deciding whether a sentences matches (supports) a key point. Therefore reducing the threshold from the 0.99 default value makes more sentences match key points and increases the coverage, at the risk of reducing the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77168e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016, kpa_top_aq_1000_2016_job_id = run_kpa(sentences_2016_top_1000_aq, \n",
    "                                                                {'n_top_kps': 20, 'mapping_threshold': 0.95})\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35101b04",
   "metadata": {},
   "source": [
    "The coverage was indeed increased to about 50%. Let's examine the bottom 5 sentences that were matched to the first key point and make sure that the precision is still high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c141bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_bottom_matches_for_kp\n",
    "print_bottom_matches_for_kp(kpa_result_top_aq_1000_2016, 'Traffic congestion needs major improvement', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d124cd3",
   "metadata": {},
   "source": [
    "## 3. Run *Key Point Analysis* over 2017 survey using the key points from 2016 survey\n",
    "### 3.1 Select top 1000 sentences from 2017 data using the *Argument Quality* service\n",
    "It is very useful to be able to compare between different subsets of the data (compare between different years, different districts, etc'). We will demonstrate how easy it is to compare the 2017 data to the 2016 data. A similar comparisson can be done between districts or other subsets. \n",
    "\n",
    "Let's first filter the 2017 sentences and take the top 1000 quality sentences, as done for the 2016 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2017 = [sentence for sentence in sentences if sentence['year'] == '2017']\n",
    "sentences_2017_top_1000_aq = get_top_quality_sentences(sentences_2017, 1000, 'Austin is a great place to live')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3941b3f",
   "metadata": {},
   "source": [
    "### 3.2 Run *Key Point Analysis* over top 1000 quality 2017 sentences using the key points from 2016\n",
    "Exercise 2:<br/>\n",
    "In order to compare the 2017 sentences to 2016 sentences we will want to map the 2017 sentences to the same key points extracted on the 2016 sentences (otherwise different key points could be automattically extracted on the 2017 sentences and it would be hard to compare between them).\n",
    "\n",
    "For this end we will reimplement the *run_kpa* method (please copy paste the previous one and modify it). This time the method will receive a new *key_points_by_job_id* parameter. This parameter is passed to the *key_points_by_job_id* parameter in the **future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids, run_params=run_params, key_points_by_job_id=key_points_by_job_id)** method. When *None* is passed to *key_points_by_job_id*, key points are automatically extracted, however when it is set with a *job_id* of a previous job it uses the key points from that job and matches all sentences to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48cec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, run_params, key_points_by_job_id=None):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain=domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params=run_params,\n",
    "                                                    key_points_by_job_id=key_points_by_job_id)\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    \n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200448e9",
   "metadata": {},
   "source": [
    "Let's use the new *run_kpa* and provide it with the *top 1000 quality sentences from 2017* and the job_id of *top 1000 quality sentences from 2016*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2017, _ = run_kpa(sentences_2017_top_1000_aq, \n",
    "                                    {'n_top_kps': 20, 'mapping_threshold': 0.95}, kpa_top_aq_1000_2016_job_id)\n",
    "print_results(kpa_result_top_aq_1000_2017, n_sentences_per_kp=2, title='Top aq 2017, using 2016 key points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0544942",
   "metadata": {},
   "source": [
    "Since both jobs have the same key points, we can now easily compare the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a53743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from austin_utils import compare_results\n",
    "\n",
    "compare_results(kpa_result_top_aq_1000_2016, '2016', kpa_result_top_aq_1000_2017, '2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0c604",
   "metadata": {},
   "source": [
    "Note: This comparision is for illustration only. Given that we ran on a subset of comments, the statistical significant of difference between the years is limited, except for the most recurring keypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06babea",
   "metadata": {},
   "source": [
    "## 4. Deep dive into the *traffic problem* in Austin using the *Term Wikifier* and *Term Relater* services\n",
    "As we've seen in the 2016 results, that the traffic problem in Austin is significant. In this section we will use the *Term Wikifier* and *Term Relater* services to select a subset of the sentences related to the *Traffic* topic and run *Key Point Analysis* over them. \n",
    "\n",
    "The *Term Wikifier* service runs over sentences and identifies the Wikipedia concepts that are referenced by phrases in the sentence text.  Concepts correspond to Wikipedia articles.  Each occurance of a concept in the sentence is called a *mention*.  For example, the sentence \"My car insurance went up 20% due to vehicle thefts and burglary\" mentions three Wikipedia concepts: The phrase \"car insurance\" is mapped to the concept *Vehicle insurance*; the phrase \"vehicle thefts\" is mapped to the concept *Motor vehicle theft* and the phrase \"burglary\" is mapped to the concept *Burglary*.\n",
    "\n",
    "The *Term Relater* service runs over pairs of Wikipedia concepts and scores how closely these concepts are related.  For example, the *Car* concept is very related to the *Traffic* concept but the *Cat* concept is not very related to the *Traffic* concept.\n",
    "\n",
    "We will use the *Term Wikifier* to extract all mentions in all sentences; then use the *Term Relater* to select a subset of these mentions which are related to the 'Traffic' concept; then select all sentences that have mentions related to the 'Traffic' concept; and finally run *Key Point Analysis* over them. Running over these sentences will create key points specifically to the traffic problem in Austin and expose insights and suggestions related to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7e298",
   "metadata": {},
   "source": [
    "### 4.1 Calculate the mentions in the sentences using the *Term Wikifier*\n",
    "Exercise 3:\n",
    "\n",
    "Please complete the missing parts in the *get_sentence_to_mentions(sentences_texts)* method. The method uses the *Term Wikifier* service, calculates the mentions for each sentence and stores it in a dictionary named *sentence_to_mentions*. \n",
    "\n",
    "The *Term Wikifier* client runs over the sentences_texts using the **mentions_list = term_wikifier_client.run(sentences_texts)** method and returns a list of mentions_lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a69177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_to_mentions(sentences_texts):\n",
    "    term_wikifier_client = debater_api.get_term_wikifier_client()\n",
    "\n",
    "    mentions_list = term_wikifier_client.run(sentences_texts)\n",
    "    \n",
    "    sentence_to_mentions = {}\n",
    "    for sentence_text, mentions in zip(sentences_texts, mentions_list):\n",
    "        sentence_to_mentions[sentence_text] = set([mention['concept']['title'] for mention in mentions])\n",
    "    return sentence_to_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de67967",
   "metadata": {},
   "source": [
    "Let's calculate the mentions on all 2016 sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4aede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2016_texts = [sentence['text'] for sentence in sentences_2016]\n",
    "sentence_to_mentions = get_sentence_to_mentions(sentences_2016_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629cc2f",
   "metadata": {},
   "source": [
    "### 4.2 Find the mentions that relate to the *traffic* concept using the *Term Relater* service\n",
    "Since we're interested in the *Traffic* concept, we will now take all mentions and find the ones that are related to that concept. Then we will select all sentences that have at least one mention that is related to the *Traffic* concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mentions = set([mention for sentence in sentence_to_mentions \n",
    "                   for mention in sentence_to_mentions[sentence]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dfce4",
   "metadata": {},
   "source": [
    "Exercise 4:<br/>\n",
    "Please complete the missing parts in the *get_related_mentions(concept, threshold, all_mentions)* method. It receives a given concept, a threshold and all_mentions. It then uses the *Term Relater* service to calculate the relatedness between the mentions and the concept and returns all mentions that have relatedness score above the given threhold. The *term_relater_client* runs over the pairs using the **scores = term_relater_client.run(concept_mention_pairs)** method and returns a list of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d236cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_mentions(concept, threshold, all_mentions):\n",
    "    term_relater_client = debater_api.get_term_relater_client()\n",
    "    concept_mention_pairs = [[concept, mention] for mention in all_mentions]\n",
    "\n",
    "    scores = term_relater_client.run(concept_mention_pairs)\n",
    "    \n",
    "    return [mention for mention, score in zip(all_mentions, scores) if score > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357e094",
   "metadata": {},
   "source": [
    "We will now use the method you've implemented and find the mentions that match the *traffic* concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b371f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_mentions = get_related_mentions('Traffic', 0.5, all_mentions)\n",
    "print(matched_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9122e5",
   "metadata": {},
   "source": [
    "### 4.3 Run *Key Point Analysis* over the sentences that relate to the *Traffic* concept\n",
    "Let's select the sentences that have mentions that are related to the *Traffic* concept and run over them. We will need to switch back from sentences_texts to sentences dictionaries since our *run_kpa* method needs the sentences dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences_texts = [sentence for sentence in sentences_2016_texts \n",
    "                     if len(sentence_to_mentions[sentence].intersection(matched_mentions)) > 0]\n",
    "matched_sentences = [sentence for sentence in sentences_2016 if sentence['text'] in matched_sentences_texts]\n",
    "matched_sentences = matched_sentences if len(matched_sentences) <= 1000 else random.sample(matched_sentences, 1000)\n",
    "print('Running over %d sentences' % len(matched_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317abe1",
   "metadata": {},
   "source": [
    "Finally, let's run over these sentences and examine the *Traffic* related key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aec329",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_traffic_2016, _ = run_kpa(matched_sentences, {'n_top_kps': 20, 'mapping_threshold': 0.99}, None)\n",
    "print_results(kpa_result_traffic_2016, n_sentences_per_kp=2, title='Traffic KPA 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8899d",
   "metadata": {},
   "source": [
    "### 4.4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14496a23",
   "metadata": {},
   "source": [
    "In this tutorial, we showed how *Key Point Analysis* can provide you with detailed insights over survey data right out of the box - significantly reducing the effort required by a data scientist to analyze the data.  We also demonstrated how key point analysis over unstructured text can be combined with available structured information, to provide new views over the data.   Finally, we showed how utilizing of additional Project Debater text analysis services such as *Argument Quality*, *Term Wikifier* , and *Term Relater* can further improve the quality of the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
