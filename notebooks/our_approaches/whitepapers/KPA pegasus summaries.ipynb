{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c6bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])  # len(self.labels)\n",
    "\n",
    "      \n",
    "def prepare_data(model_name, \n",
    "                 train_texts, train_labels, \n",
    "                 val_texts=None, val_labels=None, \n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning\n",
    "  \"\"\"\n",
    "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset, tokenizer\n",
    "\n",
    "\n",
    "def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "  \"\"\"\n",
    "  Prepare configurations and base model for fine-tuning\n",
    "  \"\"\"\n",
    "  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "  if freeze_encoder:\n",
    "    for param in model.model.encoder.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "  if val_dataset is not None:\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=output_dir,           # output directory\n",
    "      num_train_epochs=2000,           # total number of training epochs\n",
    "      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n",
    "      per_device_eval_batch_size=1,    # batch size for evaluation, can increase if memory allows\n",
    "      save_steps=500,                  # number of updates steps before checkpoint saves\n",
    "      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "      evaluation_strategy='steps',     # evaluation strategy to adopt during training\n",
    "      eval_steps=100,                  # number of update steps before evaluation\n",
    "      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "      weight_decay=0.01,               # strength of weight decay\n",
    "      logging_dir='./logs',            # directory for storing logs\n",
    "      logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "      args=training_args,                  # training arguments, defined above\n",
    "      train_dataset=train_dataset,         # training dataset\n",
    "      eval_dataset=val_dataset,            # evaluation dataset\n",
    "      tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "  else:\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=output_dir,           # output directory\n",
    "      num_train_epochs=2000,           # total number of training epochs\n",
    "      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n",
    "      save_steps=500,                  # number of updates steps before checkpoint saves\n",
    "      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "      weight_decay=0.01,               # strength of weight decay\n",
    "      logging_dir='./logs',            # directory for storing logs\n",
    "      logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "      args=training_args,                  # training arguments, defined above\n",
    "      train_dataset=train_dataset,         # training dataset\n",
    "      tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "  return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283c7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "    r'D:\\backup_user\\crypto\\thesis\\my-repos\\cryptocurrencies-kpa\\data\\processed\\ARG_KP_2021\\all_complete.csv')\n",
    "df = df[[\"topic\", \"stance\", \"argument\", \"key_point\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64478e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "all_texts = []\n",
    "all_sum = []\n",
    "for i, row in df.iterrows():\n",
    "    all_texts.append(row['argument'])\n",
    "    all_sum.append(row['key_point'])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced7f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alron\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 2000\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='2000000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    176/2000000 01:02 < 199:19:12, 2.79 it/s, Epoch 0.17/2000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.565300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>7.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.495200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.564000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m train_dataset, _, _, tokenizer \u001b[38;5;241m=\u001b[39m prepare_data(model_name, train_texts, train_labels)\n\u001b[0;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m prepare_fine_tuning(model_name, tokenizer, train_dataset)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1316\u001b[0m )\n\u001b[1;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1560\u001b[0m ):\n\u001b[0;32m   1561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use XSum dataset as example, with first 1000 docs as training data\n",
    "#from datasets import load_dataset\n",
    "#dataset = load_dataset(\"xsum\")\n",
    "\n",
    "train_texts, train_labels = all_texts[:1000], all_sum[:1000]\n",
    "\n",
    "# use Pegasus Large model as base for fine-tuning\n",
    "model_name = 'google/pegasus-large'\n",
    "train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)\n",
    "trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374eb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning of KPA failed due to gpu memory limitation, looks it takes very long time to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "108c7f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fcc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "summaries = []\n",
    "batch_size = 1\n",
    "for i in range(0, len(all_texts), batch_size):\n",
    "    batch = tokenizer(all_texts[i:i+batch_size], truncation=True, padding=\"longest\", return_tensors=\"pt\" ).to(device)\n",
    "    translated = model.generate(**batch)\n",
    "    summaries.append(translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df70cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for s in summaries:\n",
    "    tgt_text = tokenizer.batch_decode(s, skip_special_tokens=True)\n",
    "    all_preds.extend(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15368ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_test_df['pegasus_sum'] = all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c3d15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The vow of celibacy should not be abandoned.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpa_test_df.iloc[4]['pegasus_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab59d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_test_df.to_csv('./PPLM/kpa_dataset/processed_kpa.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deaa2545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>stance</th>\n",
       "      <th>argument</th>\n",
       "      <th>key_point</th>\n",
       "      <th>extracted_kps</th>\n",
       "      <th>pegasus_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assisted suicide should be a criminal offence</td>\n",
       "      <td>-1</td>\n",
       "      <td>{'if a patient is suffering with cancer or oth...</td>\n",
       "      <td>{'Assisted suicide reduces suffering', 'People...</td>\n",
       "      <td>if a patient is suffering with cancer or other...</td>\n",
       "      <td>People have the right to die with dignity and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assisted suicide should be a criminal offence</td>\n",
       "      <td>1</td>\n",
       "      <td>{'assisted suicide is killing by another name ...</td>\n",
       "      <td>{'Assisted suicide should not be allowed becau...</td>\n",
       "      <td>assisted suicide is killing by another name   ...</td>\n",
       "      <td>What do you think about assisted suicide?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homeschooling should be banned</td>\n",
       "      <td>-1</td>\n",
       "      <td>{'homeschooling removes bullying from a childs...</td>\n",
       "      <td>{'Homeschools can be personalized to the child...</td>\n",
       "      <td>homeschooling removes bullying from a childs l...</td>\n",
       "      <td>Should homeschooled children be allowed to att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homeschooling should be banned</td>\n",
       "      <td>1</td>\n",
       "      <td>{'homeschooling is not government regulated ',...</td>\n",
       "      <td>{'Homeschools cannot be regulated standardized...</td>\n",
       "      <td>homeschooling is not government regulated. the...</td>\n",
       "      <td>What is the best way to educate a child at home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The vow of celibacy should be abandoned</td>\n",
       "      <td>-1</td>\n",
       "      <td>{'celibacy before marriage protects young peop...</td>\n",
       "      <td>{'Religious experiences and traditions should ...</td>\n",
       "      <td>celibacy before marriage protects young people...</td>\n",
       "      <td>The vow of celibacy should not be abandoned.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           topic  stance  \\\n",
       "0  Assisted suicide should be a criminal offence      -1   \n",
       "1  Assisted suicide should be a criminal offence       1   \n",
       "2                 Homeschooling should be banned      -1   \n",
       "3                 Homeschooling should be banned       1   \n",
       "4        The vow of celibacy should be abandoned      -1   \n",
       "\n",
       "                                            argument  \\\n",
       "0  {'if a patient is suffering with cancer or oth...   \n",
       "1  {'assisted suicide is killing by another name ...   \n",
       "2  {'homeschooling removes bullying from a childs...   \n",
       "3  {'homeschooling is not government regulated ',...   \n",
       "4  {'celibacy before marriage protects young peop...   \n",
       "\n",
       "                                           key_point  \\\n",
       "0  {'Assisted suicide reduces suffering', 'People...   \n",
       "1  {'Assisted suicide should not be allowed becau...   \n",
       "2  {'Homeschools can be personalized to the child...   \n",
       "3  {'Homeschools cannot be regulated standardized...   \n",
       "4  {'Religious experiences and traditions should ...   \n",
       "\n",
       "                                       extracted_kps  \\\n",
       "0  if a patient is suffering with cancer or other...   \n",
       "1  assisted suicide is killing by another name   ...   \n",
       "2  homeschooling removes bullying from a childs l...   \n",
       "3  homeschooling is not government regulated. the...   \n",
       "4  celibacy before marriage protects young people...   \n",
       "\n",
       "                                         pegasus_sum  \n",
       "0  People have the right to die with dignity and ...  \n",
       "1          What do you think about assisted suicide?  \n",
       "2  Should homeschooled children be allowed to att...  \n",
       "3   What is the best way to educate a child at home?  \n",
       "4       The vow of celibacy should not be abandoned.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpa_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22acfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "terra_df = pd.read_csv('./PPLM/kpa_dataset/processed_terra.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef2bd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>extracted_kps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>While many see the benefits of a price-stable ...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>While many see the benefits of a price-stable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The price-volatility of cryptocurrencies is a ...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>The price-volatility of cryptocurrencies is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A stable-coin mechanism must answer three key ...</td>\n",
       "      <td>Multi-fiat peg monetary policy</td>\n",
       "      <td>A stable-coin mechanism must answer three key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The existential objective of a stable-coin is ...</td>\n",
       "      <td>Defining stability against regional fiat curre...</td>\n",
       "      <td>The existential objective of a stable-coin is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since the price of Terra currencies in seconda...</td>\n",
       "      <td>Measuring stability with miner oracles</td>\n",
       "      <td>Since the price of Terra currencies in seconda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  While many see the benefits of a price-stable ...   \n",
       "1  The price-volatility of cryptocurrencies is a ...   \n",
       "2  A stable-coin mechanism must answer three key ...   \n",
       "3  The existential objective of a stable-coin is ...   \n",
       "4  Since the price of Terra currencies in seconda...   \n",
       "\n",
       "                                             section  \\\n",
       "0                                           Abstract   \n",
       "1                                       Introduction   \n",
       "2                     Multi-fiat peg monetary policy   \n",
       "3  Defining stability against regional fiat curre...   \n",
       "4             Measuring stability with miner oracles   \n",
       "\n",
       "                                       extracted_kps  \n",
       "0  While many see the benefits of a price-stable ...  \n",
       "1  The price-volatility of cryptocurrencies is a ...  \n",
       "2  A stable-coin mechanism must answer three key ...  \n",
       "3  The existential objective of a stable-coin is ...  \n",
       "4  Since the price of Terra currencies in seconda...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terra_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0657e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "summaries = []\n",
    "all_texts = terra_df['text'].tolist()\n",
    "batch_size = 5\n",
    "for i in range(0, len(all_texts), batch_size):\n",
    "    batch = tokenizer(all_texts[i:i+batch_size], truncation=True, padding=\"longest\", return_tensors=\"pt\" ).to(device)\n",
    "    translated = model.generate(**batch)\n",
    "    summaries.append(translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aea9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for s in summaries:\n",
    "    tgt_text = tokenizer.batch_decode(s, skip_special_tokens=True)\n",
    "    all_preds.extend(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fffaad75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The adoption of cryptocurrencies is on the rise.',\n",
       " 'In this paper, we present the Terra Protocol, an elastic monetary policy for cryptocurrencies.',\n",
       " 'A stable-coin is an asset with a long-term track record of price stability.',\n",
       " 'The Terra Protocol aims to create a stable-coin ecosystem.',\n",
       " 'A price oracle is a key part of the Terra protocol.',\n",
       " 'The Terra money market is a pegged fiat system in which the price of a Terra currency is pegged to the price of money.',\n",
       " 'The Terra Protocol is a Proof of Stake (PoS) blockchain, where miners need to stake a native cryptocurrency Luna to mine Terra transactions. The Terra Protocol runs on a Proof of Stake (PoS) blockchain, where miners need to stake a native cryptocurrency Luna to mine Terra transactions.',\n",
       " 'The Terra protocol aims to provide stable and predictable rewards to miners.',\n",
       " 'The Terra Platform DApps will offer a stable platform for building financial applications that use Terra as their underlying currency. Terra will offer a stable dApp platform oriented to building financial applications that use Terra as their underlying currency, thus allowing smart contracts to mature into a useful infrastructure for businesses.',\n",
       " 'Terra is a digital currency that is designed to complement both existing fiat and cryptocurrencies as a way to transact and store value.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4aaba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "terra_df['pegasus_sum'] = all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da06c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "terra_df.to_csv('./PPLM/kpa_dataset/processed_terra.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc5f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kpa = pd.read_csv('./PPLM/kpa_dataset/processed_kpa.csv', index_col=False)\n",
    "btc = pd.read_csv('./PPLM/kpa_dataset/processed_btc.csv', index_col=False)\n",
    "terra = pd.read_csv('./PPLM/kpa_dataset/processed_terra.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0df67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the best way to educate a child at home?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpa.iloc[3]['pegasus_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2fc6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Terra is a digital currency that is designed to complement both existing fiat and cryptocurrencies as a way to transact and store value.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terra.iloc[9]['pegasus_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f8c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis] *",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
